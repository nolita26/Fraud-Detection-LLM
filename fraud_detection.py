# -*- coding: utf-8 -*-
"""fraud_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vXafRBPqbF9YTAoS0X5tsxh4guewN-Ej
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from transformers import BertTokenizer, TFBertForSequenceClassification
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# Load and preprocess the dataset
def load_data(path):
    data = pd.read_csv(path)
    # Preprocess the data as needed (e.g., handle missing values, feature engineering)
    return data

# Tokenize text data using BERT tokenizer
def tokenize_text(text_data):
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    tokenized_text = tokenizer(text_data.tolist(), padding=True, truncation=True, return_tensors='tf')
    return tokenized_text

# Split the dataset into train and test sets
def split_data(data):
    X_train, X_test, y_train, y_test = train_test_split(data.drop(columns=['target']), data['target'], test_size=0.2, random_state=42)
    return X_train, X_test, y_train, y_test

# Train BERT-based model for sequence classification
def train_model(X_train, y_train):
    model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    model.fit(X_train, y_train, epochs=3, batch_size=32)
    return model

# Evaluate the model on test data
def evaluate_model(model, X_test, y_test):
    predictions = model.predict(X_test)
    # Perform evaluation metrics (e.g., classification report, confusion matrix)
    print(classification_report(y_test, predictions))

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

# Load train CSV file
train_df = pd.read_csv('/content/fraudTrain.csv')

# Load test CSV file
test_df = pd.read_csv('/content/fraudTest.csv')

# Main function to run the project
def main():
    # Load and preprocess the dataset
    data = load_data('fraudTrain.csv')
    # Tokenize text data
    tokenized_data = tokenize_text(data['merchant'])
    # Split the dataset into train and test sets
    X_train, X_test, y_train, y_test = split_data(tokenized_data)
    # Train the BERT-based model
    model = train_model(X_train, y_train)
    # Evaluate the model on test data
    evaluate_model(model, X_test, y_test)

if __name__ == "__main__":
    main()

